{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# __Simple Machine Learning Project: A Walkthrough__\r\n",
    "## __Table of Contents__:\r\n",
    "### 0.) Dataset Description\r\n",
    "### 1.) Data Cleaning\r\n",
    "### 2.) Feature Engineering\r\n",
    "### 3.) Feature Selection\r\n",
    "### 4.) Model Performance\r\n",
    "\r\n",
    "## __Data Description__:\r\n",
    "For this walkthrough we'll be using the Concrete Dataset found [here](https://www.kaggle.com/maajdl/yeh-concret-data). We'll be predicting the Compressive Strength of Concrete given its Age and its ingredients. Some Features include:\r\n",
    "Water(kg), Cement(kg), Age(days) etc. <blockquote> \"Concrete is the most important material in civil engineering. Concrete compressive strength is a highly nonlinear function of age and ingredients.[maajdl](https://www.kaggle.com/maajdl)\"</blockquote>\r\n",
    "### __Run the cell below to check out the structure of the dataset:__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from exploratoryDataAnalysis import *\r\n",
    "_,_,df = loadData()\r\n",
    "df.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## __Data Cleaning__:\r\n",
    "Let's have a look at the data! First let's check for any missing values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thankfully, there are none. In the wild, our data will almost certainly contain missing values, which we'll either have to impute(replace with estimates) or drop from the data.\r\n",
    "Next, we'll look at some useful statistics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "summaryStatistics(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can gleen from the datatypes portion that we are only dealing with numerical data. But the summary statistics are hard to interpret, we'll look at the distribution of our features to enrich our understanding. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "featureDistributions(df)\r\n",
    "boxPlots(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cement, Slag, Ash and others look left skewed, and those dots outside of the minimum and maximum lines of the box plots are outliers. The distributions of our features are not Gaussian(normal)-like. The success Machine Learning algorithms depends on the features being normally distributed. The method we will use to detect outliers relies on this assumption as well. So we'll go ahead transform our features to make them normally distributed by taking their square root, the log transform is also very popular but doesn't work when features contains 0's."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from featureEngineering import *\r\n",
    "df = transformFeatures(df)\r\n",
    "df.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll wrap up the data cleaning by removing outliers. We'll do that by taking the Z-score:(x-mean)/standardDeviation of the features. The Z-score will tell us how many standard deviations away our data is from the mean. If it's too far, in our case greater than 2.5, we'll remove it from our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(df))\r\n",
    "_,_,df = findAndRemoveOutliers(df)\r\n",
    "print(len(df))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, we've dropped a few rows from our database.\r\n",
    "## __Feature Engineering__:\r\n",
    "Now that our data is clean, we can begin Feature Engineering (technically, our sqrt transform was feature engineering). In our case, this will only involve feature creation. We will combine our features in interesting ways to create new features. In principle, some of these features would be hard for our ML algorithms to find without a little help.\r\n",
    "We will take every product and ratio of every pair of features, and add them to our dataset as new features.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = featureCreation(df)\r\n",
    "df.columns.tolist()[15:30]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}